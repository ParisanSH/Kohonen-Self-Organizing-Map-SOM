# Kohonen-Self-Organizing-Map-SOM-
In this mini project, I have tried to train and test a SOM network to do cluster analysis of news collection, from the BBC news website corresponding to stories in five topical areas from 2004-2005. The dataset is a collection of 2225 news documents, categorized into 5 classes â€˜businessâ€™, â€˜entertainmentâ€™, â€˜politicsâ€™, â€˜sportâ€™, and â€˜techâ€™.


Text Clustering using SOM
Text clustering is an unsupervised process, used to separate a document collection into some clusters on the basis of the similarity relationship between documents in the collection. Suppose ğ· = {ğ‘‘1, â€¦ , ğ‘‘ğ‘} be a collection of ğ‘ documents to be clustered. The task is to divide ğ· into ğ‘˜ clusters
ğ¶1, â€¦ , ğ¶ğ‘˜ where ğ¶1 âˆª â€¦ âˆª ğ¶ğ‘˜ = ğ· and ğ¶ğ‘– âˆ© ğ¶ğ‘— = âˆ…, for ğ‘– â‰  ğ‘—.
SOM text clustering can be done in two main phases. 
The first phase is document preprocessing, which uses Vector Space Model (VSM) to generate a numeric vector for each text document. 
In the next phase, SOM is applied on the document vectors to obtain document clusters.

Phase 1: Document Preprocessing

By means of VSM, each document ğ‘‘ğ‘– can be represented by an ğ‘›-dimensional feature vector ğ’—ğ‘– =< ğ‘£ğ‘–1, â€¦ , ğ‘£ğ‘–ğ‘› >, where ğ‘£ğ‘–ğ‘— is a representation of term ğ‘¡ğ‘— in document ğ‘‘ğ‘– and ğ‘› is the number of distinct terms in the document collection ğ·.

An approach for computing ğ‘£ğ‘–ğ‘— is the Term Frequency - Inverse Document Frequency (TF-IDF) weighting scheme. This method computes ğ‘£ğ‘–ğ‘— for term ğ‘¡ğ‘— in document ğ‘‘ğ‘– as:
ğ‘£ğ‘–ğ‘— = log(1 + ğ‘¡ğ‘“ğ‘–ğ‘—) Ã— log(ğ‘/ğ‘‘ğ‘“ğ‘—)
where ğ‘¡ğ‘“ğ‘–ğ‘— is the frequency of term ğ‘¡ğ‘— in document ğ‘‘ğ‘–, and ğ‘‘ğ‘“ğ‘— is the number of documents in ğ· containing term ğ‘¡ğ‘—.

First, the â€˜BBC-text.csvâ€™ file read and for each document:
1. Removing all non-letter characters from the documents.
2. Extracting all words of the document and removing the short words (length â‰¤ 2).
3. Removing all stop words (e.g., â€˜aâ€™, â€˜andâ€™, â€˜whatâ€™, â€¦), given in file â€˜stopwords.txtâ€™.
4. Computing the feature vector for each document, using the TF-IDF weighting scheme.

Phase 2: SOM Clustering
a) Winner-takes-all approach
1. Using all documents to build a SOM with one neuron for each class.
2. Depicting the SOM-hits plot.
3. Computing and reporting the confusion matrix.

b) On-center, off-surround approach
1. Using all documents to build a SOM with 3*3 neurons.
2. Depicting the SOM-hits plot.
3. Computing the Euclidean distance of all documents to their winner neurons and summing up the distances.
4. Repeating steps 1-3 for 4*4 and 5*5 topologies.

